{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzRuEqaSxbfH"
      },
      "source": [
        "#Downloading The Required Packages:\n",
        "- *datasets*: To download and prepare our data.\n",
        "- *transformers* and *accelerate*: To load and train our Whisper model.\n",
        "- *soundfile* and *librosa*: To pre-process audio files. (used internally)\n",
        "- *evaluate* and *jiwer*: To measure the performance of our model. (used internally)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CpwNIAMrp6Q"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install soundfile\n",
        "!pip install librosa\n",
        "!pip install evaluate\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGGHL0lpzV8Z"
      },
      "source": [
        "# Loading The Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XSES8lX2blr"
      },
      "outputs": [],
      "source": [
        "!cd /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIyfIAmw1kbx"
      },
      "source": [
        "You need to login to Hugging Face to download some models and datasets that require accepting their terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziAyCRbD29eU"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrQBSg1KzZfd"
      },
      "source": [
        "##Loading Common Voice 13:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_-t8G4UzVIq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "# Combine both training and validation splits into one since Arabic dataset is small\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"ar\", split=\"train+validation\")\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"ar\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK96ndd13GRs"
      },
      "outputs": [],
      "source": [
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCeTPAWv36uz"
      },
      "outputs": [],
      "source": [
        "# Removing unwanted features (we only want audio and its transcription)\n",
        "common_voice = common_voice.remove_columns([\"client_id\", \"path\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accent\", \"locale\", \"segment\", \"variant\"])\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "insePPLw-9OZ"
      },
      "outputs": [],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBFuhVKm-P9d"
      },
      "source": [
        "# Preparing The dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ZsgE947RF4"
      },
      "source": [
        "## Preparing Feature Extractor & Tokenizer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASZHGtdE7f3q"
      },
      "source": [
        "\n",
        "\n",
        "* Feature Extractor:\n",
        "  * Transforms audio into 30s clips either by splitting them if longer than 30s or adding silence if less than 30s.\n",
        "  This is essential since audio files can have different durations and thus different and this can affect the extracted features length for each audio\n",
        "  * Transforms audio to log-mel spectogram which the model expects as input.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx27IaqM8S7b"
      },
      "source": [
        "\n",
        "\n",
        "* Tokenizer:\n",
        "  * Transforms the output of the model (token IDs) to their respective text.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGjuulxn8OJf"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "# WhisperProcesor combines both feature extractor and tokenizer\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Arabic\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C99QXcXV-fs9"
      },
      "source": [
        "## Getting Dataset Ready:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7hjboGh9dlQ"
      },
      "outputs": [],
      "source": [
        "# We need to change the sample rate from 48KHz to 16KHz since this is what whisper expects\n",
        "from datasets import Audio\n",
        "\n",
        "# cast_column makes datasets perform the resampling on the fly when the data is loaded\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avwtb9fg-su6"
      },
      "outputs": [],
      "source": [
        "# loading it into memory like this will automatically cast it to 16KHz\n",
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the encoding and decoding\n",
        "sentence = common_voice[\"train\"][0][\"sentence\"]\n",
        "labels = processor.tokenizer(sentence)\n",
        "decoded_with_special = processor.tokenizer.decode(labels.input_ids, skip_special_tokens=False)\n",
        "decoded = processor.tokenizer.decode(labels.input_ids, skip_special_tokens=True)\n",
        "print(decoded_with_special)\n",
        "print('*' * 100)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "FqhdVKoOLldd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s84lK72V_NB6"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(data_item):\n",
        "    # loading the data item to resample it\n",
        "    audio = data_item[\"audio\"]\n",
        "    sentence = data_item[\"sentence\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array and add it to our item\n",
        "    data_item[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"])[\"input_features\"][0]\n",
        "\n",
        "    # encode target text to label ids and add it to our items\n",
        "    data_item[\"labels\"] = processor.tokenizer(sentence)[\"input_ids\"]\n",
        "\n",
        "    # the returned item will only have input_features and labels\n",
        "    return data_item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9OyCO7zALMj"
      },
      "outputs": [],
      "source": [
        "# apply prepare_dataset function to all the training data and remove the original columns (audio and sentence)\n",
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input features are the same length, but labels aren't\n",
        "print(len(common_voice[\"train\"][0][\"input_features\"]))\n",
        "print(len(common_voice[\"train\"][1][\"input_features\"]))\n",
        "print(len(common_voice[\"train\"][2][\"input_features\"]))\n",
        "print(len(common_voice[\"train\"][0][\"labels\"]))\n",
        "print(len(common_voice[\"train\"][1][\"labels\"]))\n",
        "print(len(common_voice[\"train\"][2][\"labels\"]))"
      ],
      "metadata": {
        "id": "LUMLNauDkhAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debugging data collator\n",
        "def testing_func(data_item):\n",
        "    input_features = [{\"input_features\": feature[\"input_features\"]} for feature in data_item]\n",
        "\n",
        "    print(len(input_features))\n",
        "    print(type(input_features))\n",
        "    print(type(input_features[0]))\n",
        "    print(input_features[0].keys())\n",
        "    print('*' * 100)\n",
        "\n",
        "    batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "    print(\"size of audio feature vector before padding: \" + str(len(data_item[0][\"input_features\"])))\n",
        "    print(\"size of audio feature vector after padding: \" + str(len(batch[\"input_features\"][0])))\n",
        "    print('*' * 100)\n",
        "\n",
        "    print(len(batch[\"input_features\"]))\n",
        "    print(type(batch))\n",
        "    print(type(batch[\"input_features\"]))\n",
        "    print(batch.keys())\n",
        "    print(batch[\"input_features\"])\n",
        "    print('*' * 100)\n",
        "\n",
        "\n",
        "    label_features = [{\"input_ids\": feature[\"labels\"]} for feature in data_item]\n",
        "    labels_batch = processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "    print(\"size of labels vector 1 before padding: \" + str(len(data_item[0][\"labels\"])))\n",
        "    print(\"size of labels vector 2 before padding: \" + str(len(data_item[1][\"labels\"])))\n",
        "    print(\"size of labels vector 1 before padding: \" + str(len(labels_batch[\"input_ids\"][0])))\n",
        "    print(\"size of labels vector 2 before padding: \" + str(len(labels_batch[\"input_ids\"][1])))\n",
        "    print('*' * 100)\n",
        "\n",
        "    print(labels_batch.keys())\n",
        "    print(type(labels_batch[\"input_ids\"]))\n",
        "    print(labels_batch[\"input_ids\"][0])\n",
        "    print(labels_batch[\"input_ids\"][1])\n",
        "    print(type(labels_batch[\"attention_mask\"]))\n",
        "    print(labels_batch[\"attention_mask\"][0])\n",
        "    print(labels_batch[\"attention_mask\"][1])\n",
        "    print('*' * 100)\n",
        "\n",
        "    labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"attention_mask\"].ne(1), -100)\n",
        "\n",
        "    print(labels[0])\n",
        "    print(labels[1])\n",
        "    print('*' * 100)\n",
        "\n",
        "    print(labels[:,0])\n",
        "    print(processor.tokenizer.bos_token_id)\n",
        "    print('*' * 100)\n",
        "\n",
        "    if (labels[:, 0] == processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "        print(\"Entered\")\n",
        "        labels = labels[:, 1:]\n",
        "    print(labels[0])\n",
        "    print(labels[1])\n",
        "    print(processor.tokenizer.bos_token_id)\n",
        "    print('*' * 100)\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    print(batch.keys())\n",
        "    print(batch[\"labels\"][0])\n",
        "testing_func([common_voice[\"train\"][0], common_voice[\"train\"][1]])"
      ],
      "metadata": {
        "id": "PdrsyX2jUyNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kPkNiR5DRpY"
      },
      "outputs": [],
      "source": [
        "# creating a class to get the data and batch it\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorWhisper:\n",
        "    processor: Any\n",
        "\n",
        "    # data will be passed to this function\n",
        "    def __call__(self, data_batch: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply converting them to PyTorch tensors and nothing more\n",
        "        # no padding will be done since all input_features are padded to 30s and converted to a log-Mel spectrogram of fixed dimension before\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in data_batch] # list of features where each element is the dictionary containing the feature vector of a data item from the data batch\n",
        "\n",
        "        # pad() searches for the longest input features vector and pads the rest to be just like it in length, \"pt\" means PyTorch which indicates the returned feature as PyTorch tensor\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\") # dictionary containing a list of audio features as PyTorch tensors.\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in data_batch]\n",
        "        # pad the labels to max length to make them all have the same length\n",
        "        # for two audio files with input_id vectors of length 16 and 23, after padding, an attention_mask is created\n",
        "        # attention_mask will contain two vectors coinciding with the two vectors of input_ids\n",
        "        # their length is 23 each containing 1s and 0s, 0s at an index means that these elements have been padded at that index\n",
        "        # so, the first attention_mask vector which corresponds to input_id 16, will have 0s starting from index 16 till 22\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore these tokens when calculating loss according to whisper requirements\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"attention_mask\"].ne(1), -100)\n",
        "\n",
        "        # if beginning of sequence (bos) token is appended in previous tokenization step\n",
        "        # remove it here; as it's appended later\n",
        "        # .all checks if this condition is true for all sequences in the batch\n",
        "        # .cpu().item() converts the result from a tensor to a boolean to evaluate the if condition\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaw4mXVVNoNm"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWhisper(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning:"
      ],
      "metadata": {
        "id": "BQwymy4pEw5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metric:\n",
        "We will use WER for evaluation."
      ],
      "metadata": {
        "id": "Qj_XSF29E3bc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VN0fNoRNoRH"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metric)"
      ],
      "metadata": {
        "id": "e2pc0eCjF100"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id to allow the decoder to decode them back to strings (so that it doesn't try to decode -100 back to a string)\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "SPG_keUaF12-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Pre-trained Model:"
      ],
      "metadata": {
        "id": "5Iw4jaXqBL7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
      ],
      "metadata": {
        "id": "5ehQ1FbZF16V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are fine tuning on a specific language, we need to remove some of the restrictions of the model's generation.\n",
        "\n",
        "\n",
        "\n",
        "* Forced Decoder IDs:\n",
        "  They control the transcription language and task for zero-shot automatic speech recognition (ASR). It is a mechanism to pre-determine or force certain token IDs as outputs of the model before the autoregressive generation process begins (generating output one token at a time). It is a way to control the starting point of output generation. Essentially, it's like telling the model, \"Start your output with these specific tokens, and then continue generating the rest.\". It can be like specifying the language to generate or the type of generation task like transcription. By setting this to None, we are configuring the model to not force any specific token IDs during the generation (prediction) process which is decoding (predicting while training) the token IDs to produce the text.\n",
        "\n",
        "* Suppress Tokens:\n",
        "This deals with tokens that the model should never generate, like harmful and inappropriate tokens. When setting it to an empty list, we're indicating that no tokens should be suppressed. This is a form of configuration that ensures all possible tokens can be sampled during the generation, which is often desired during the fine-tuning process.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wF_pcqMdHyqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "j0gaSEykDLzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining The Training Configuration:"
      ],
      "metadata": {
        "id": "mxkyWTpsLg2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./content/whisper-tiny-ar\",\n",
        "    per_device_train_batch_size=16, # batch size for training per GPU/CPU\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500, # linear warmup (from 0 to learning_rate)\n",
        "    max_steps=4000, # a step a batche of data will be processed the model parameters will be updated based on that batch. 4000 steps will be processed regardless of the number of epochs.\n",
        "    gradient_checkpointing=True, # saves memory by recomputing some activations during the backward pass instead of storing all the activation values. This takes more time.\n",
        "    fp16=True, # use 16-bit mixed precision during training instead of 32\n",
        "    evaluation_strategy=\"steps\", # evaluation is done every eval_steps\n",
        "    eval_steps=1000,\n",
        "    per_device_eval_batch_size=8, # batch size for evaluation per GPU/CPU\n",
        "    predict_with_generate=True, # allows the model to generate entire sequences for evaluation, instead of just single tokens\n",
        "    generation_max_length=225, # the max number of tokens to be generated during evaluation\n",
        "    save_steps=1000, # a checkpoint is saved every 1000 steps\n",
        "    logging_steps=25, # when to receive logs\n",
        "    load_best_model_at_end=True, # the best model will be loaded at the end of training\n",
        "    metric_for_best_model=\"wer\", # this metric will be used when comparing different models during training to get the best model\n",
        "    greater_is_better=False, # it means that a lower value for WER indicates that a model is better\n",
        ")"
      ],
      "metadata": {
        "id": "RxgN4x7GLYZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "zZ3BpMEIWJH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "KrHpRkQhpj7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "PiSMH_0Rps2o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
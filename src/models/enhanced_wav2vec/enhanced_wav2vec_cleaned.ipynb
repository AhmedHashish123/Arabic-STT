{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Installing & Importing Necessary Packages & Wav2Vec Model:"
      ],
      "metadata": {
        "id": "vwpRqQI3jG7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio transformers datasets"
      ],
      "metadata": {
        "id": "FvbUVAiQijJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ffwp0rWhsD5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import soundfile as sf\n",
        "import torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = Wav2Vec2Processor.from_pretrained(\"othrif/wav2vec2-large-xlsr-arabic\")\n",
        "wav2vec_model = Wav2Vec2Model.from_pretrained(\"othrif/wav2vec2-large-xlsr-arabic\")"
      ],
      "metadata": {
        "id": "mYdZijOChz-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "CWZwYNzxiq4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Downloading & Processing Datasets:"
      ],
      "metadata": {
        "id": "aJ6OsQoL0eWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading & Processing Common Voice 13 Dataset:"
      ],
      "metadata": {
        "id": "MzhbbYxp0g_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"ar\", split=\"train+validation+test\")"
      ],
      "metadata": {
        "id": "Ulv9pxsiisth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice)"
      ],
      "metadata": {
        "id": "SH_ceoAMkiaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.remove_columns([\"client_id\", \"audio\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accent\", \"locale\", \"segment\", \"variant\"])\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "UyikuA1IkicW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[0])"
      ],
      "metadata": {
        "id": "pltajkSTkif1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading & Processing Google Fleurs Dataset:"
      ],
      "metadata": {
        "id": "je1TTwfX0qWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_train = load_dataset(\"google/fleurs\", \"ar_eg\", split=\"train\")"
      ],
      "metadata": {
        "id": "QJ16pwE4GJSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_test = load_dataset(\"google/fleurs\", \"ar_eg\", split=\"test\")"
      ],
      "metadata": {
        "id": "Pxq3P14EGJUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_val = load_dataset(\"google/fleurs\", \"ar_eg\", split=\"validation\")"
      ],
      "metadata": {
        "id": "jpcgUZd4GJW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(fleurs_train))\n",
        "print(len(fleurs_test))\n",
        "print(len(fleurs_val))"
      ],
      "metadata": {
        "id": "JwfhD1wiGJac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fleurs_train)"
      ],
      "metadata": {
        "id": "zyR8CAEEGNoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_audio_path_train(data_item):\n",
        "  parts = data_item[\"path\"].split('/')\n",
        "  parts.insert(-1, \"train\")\n",
        "  data_item[\"path\"] = '/'.join(parts)\n",
        "  data_item[\"sentence\"] = data_item[\"transcription\"]\n",
        "  return data_item\n",
        "def update_audio_path_test(data_item):\n",
        "  parts = data_item[\"path\"].split('/')\n",
        "  parts.insert(-1, \"test\")\n",
        "  data_item[\"path\"] = '/'.join(parts)\n",
        "  data_item[\"sentence\"] = data_item[\"transcription\"]\n",
        "  return data_item\n",
        "def update_audio_path_val(data_item):\n",
        "  parts = data_item[\"path\"].split('/')\n",
        "  parts.insert(-1, \"dev\")\n",
        "  data_item[\"path\"] = '/'.join(parts)\n",
        "  data_item[\"sentence\"] = data_item[\"transcription\"]\n",
        "  return data_item"
      ],
      "metadata": {
        "id": "s38SrgDoGNqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_train = fleurs_train.map(update_audio_path_train)"
      ],
      "metadata": {
        "id": "kDpZl8t9GNs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_test = fleurs_test.map(update_audio_path_test)"
      ],
      "metadata": {
        "id": "HufeHUjnGNvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_val = fleurs_val.map(update_audio_path_val)"
      ],
      "metadata": {
        "id": "YlatGKT7GNxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_train = fleurs_train.remove_columns([\"id\", \"num_samples\", \"audio\", \"transcription\", \"raw_transcription\", \"gender\", \"lang_id\", \"language\", \"lang_group_id\"])"
      ],
      "metadata": {
        "id": "mdMKGD4YGNz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_test = fleurs_test.remove_columns([\"id\", \"num_samples\", \"audio\", \"transcription\", \"raw_transcription\", \"gender\", \"lang_id\", \"language\", \"lang_group_id\"])"
      ],
      "metadata": {
        "id": "CNS8YsgyGN3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_val = fleurs_val.remove_columns([\"id\", \"num_samples\", \"audio\", \"transcription\", \"raw_transcription\", \"gender\", \"lang_id\", \"language\", \"lang_group_id\"])"
      ],
      "metadata": {
        "id": "u0ZzIFUIGVy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fleurs_train[0])\n",
        "print(fleurs_test[0])\n",
        "print(fleurs_val[0])"
      ],
      "metadata": {
        "id": "Q5bNwDzWGV1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Combining Datasets & Creating The Vocabulary:"
      ],
      "metadata": {
        "id": "LrvP3RU_-2z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating The Combined Dataset For Training & The Vocabulary:"
      ],
      "metadata": {
        "id": "zEHFW9Fo-5eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = concatenate_datasets([common_voice, fleurs_train, fleurs_test])"
      ],
      "metadata": {
        "id": "5Yk4hK28GV44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(common_voice))\n",
        "print(len(fleurs_train))\n",
        "print(len(fleurs_test))\n",
        "print(len(fleurs_val))\n",
        "print(len(combined_dataset))"
      ],
      "metadata": {
        "id": "HO7LrfdxGZpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_dataset)"
      ],
      "metadata": {
        "id": "324GpfHLGZsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_dataset[0])"
      ],
      "metadata": {
        "id": "_KSyBnT9GZv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = ['ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'آ', 'أ', 'إ', 'ؤ', 'ئ', 'ة', 'ى', 'ﻻ', 'ﻷ', 'ﻹ', 'ﻵ',' ', '.']"
      ],
      "metadata": {
        "id": "kyVWmk5LH1p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing The Training & Validation Datasets:"
      ],
      "metadata": {
        "id": "jIRCNn3N_J-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_transcriptions(data_item):\n",
        "  new_sentence = ''.join([char for char in data_item[\"sentence\"] if char in vocabulary])\n",
        "  data_item[\"sentence\"] = new_sentence\n",
        "  return data_item"
      ],
      "metadata": {
        "id": "8XXOhSYWH1tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = combined_dataset.map(process_transcriptions)"
      ],
      "metadata": {
        "id": "w5ze4pS2H6R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_dataset[0])"
      ],
      "metadata": {
        "id": "QjQR3-9sH8CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_val = fleurs_val.map(process_transcriptions)"
      ],
      "metadata": {
        "id": "eHMj0J5GaYo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Creating The Dataset Class:"
      ],
      "metadata": {
        "id": "pfVSzf1A_aof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, processor):\n",
        "    self.dataset = dataset\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    audio_input, sampling_rate = torchaudio.load(self.dataset[idx][\"path\"])\n",
        "    if sampling_rate != 16000:\n",
        "      resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
        "      audio_input = resampler(audio_input)\n",
        "\n",
        "    # input_values = self.processor(audio_input.squeeze(), sampling_rate=16000, return_tensors=\"pt\").input_values.squeeze()\n",
        "    input_values = self.processor(audio_input, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
        "    input_values = input_values.squeeze(0)\n",
        "    input_length = input_values.shape[1]\n",
        "    # print(len(input_values))\n",
        "    # print(input_values)\n",
        "    # print(input_values.shape)\n",
        "    # print(input_length)\n",
        "\n",
        "    labels = self.processor.tokenizer.encode(self.dataset[idx][\"sentence\"])\n",
        "    labels = torch.tensor(labels)\n",
        "    # print(labels)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return input_values, labels, input_length"
      ],
      "metadata": {
        "id": "e5MJNopxiOJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_data_set = CustomDataset(combined_dataset, processor)\n",
        "custom_val_data_set = CustomDataset(fleurs_val, processor)"
      ],
      "metadata": {
        "id": "wxp3e6u0iOLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- Model Architecture:"
      ],
      "metadata": {
        "id": "tuHL_RnZAVoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "id": "naEHRy_digHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "4n_e9ZQ8P6Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSTTModel(nn.Module):\n",
        "  def __init__(self, wav2vec_model, lstm_hidden_size, lstm_layers, attention_heads):\n",
        "    super(CustomSTTModel, self).__init__()\n",
        "    self.wav2vec = wav2vec_model\n",
        "\n",
        "    for param in self.wav2vec.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    feature_size = self.wav2vec.config.hidden_size\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=feature_size,\n",
        "                        hidden_size=lstm_hidden_size,\n",
        "                        num_layers=lstm_layers,\n",
        "                        batch_first=True)\n",
        "\n",
        "    self.attention = nn.MultiheadAttention(embed_dim=lstm_hidden_size,\n",
        "                                            num_heads=attention_heads,\n",
        "                                            batch_first=True)\n",
        "\n",
        "    self.output_layer = nn.Linear(lstm_hidden_size, wav2vec_model.config.vocab_size)\n",
        "\n",
        "  def forward(self, input_values, input_lengths):\n",
        "    self.wav2vec.eval()\n",
        "    with torch.no_grad():\n",
        "      # print(\"input_values: \", len(input_values[0]))\n",
        "      # print(\"input_values: \", input_values[0])\n",
        "      # print(\"input_values: \", input_values[0].shape)\n",
        "      # print(\"input_lengths: \", input_lengths)\n",
        "      # print(\"vocab size: \", self.wav2vec.config.vocab_size)\n",
        "      wav2vec_output = self.wav2vec(input_values).last_hidden_state\n",
        "\n",
        "    processed_lengths = wav2vec_output.shape[1]\n",
        "    processed_lengths = torch.full((wav2vec_output.shape[0],), processed_lengths, dtype=torch.int64)\n",
        "    # print(\"processed_lengths: \", processed_lengths)\n",
        "    sorted_lengths, sorted_indices = input_lengths.sort(descending=True)\n",
        "    sorted_wav2vec_output = wav2vec_output[sorted_indices]\n",
        "\n",
        "    # print(\"feature size: \", self.wav2vec.config.hidden_size)\n",
        "    # print(\"Sorted lengths:\", sorted_lengths)\n",
        "    # print(\"Shape of wav2vec_output:\", wav2vec_output.shape)\n",
        "    # print(\"Shape of sorted_wav2vec_output:\", sorted_wav2vec_output.shape)\n",
        "\n",
        "    packed_input = pack_padded_sequence(sorted_wav2vec_output, processed_lengths.cpu(), batch_first=True)\n",
        "    # print(\"packed_input shape\", packed_input)\n",
        "    if input_values.is_cuda:\n",
        "      self.lstm.flatten_parameters()\n",
        "\n",
        "    packed_lstm_output, _ = self.lstm(packed_input)\n",
        "\n",
        "    lstm_output, _ = pad_packed_sequence(packed_lstm_output, batch_first=True)\n",
        "\n",
        "    attention_output, _ = self.attention(lstm_output, lstm_output, lstm_output)\n",
        "\n",
        "    output = self.output_layer(attention_output)\n",
        "\n",
        "    # print(\"sorted_lengths:\", sorted_lengths)\n",
        "    # print(\"sorted_wav2vec_output:\", sorted_wav2vec_output)\n",
        "    # print(\"packed_input:\", packed_input)\n",
        "    # print(\"packed_lstm_output:\", packed_lstm_output)\n",
        "    # print(\"lstm_output:\", lstm_output)\n",
        "    # print(\"attention_output:\", attention_output)\n",
        "    # print(\"output:\", output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "Wl6zvxijH7yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model = CustomSTTModel(wav2vec_model, lstm_hidden_size=128, lstm_layers=2, attention_heads=4)"
      ],
      "metadata": {
        "id": "UqIc02qnh0EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6- Training Phase:"
      ],
      "metadata": {
        "id": "0OwCFIIaBNUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating The Collate Function & Initializing Train & Validation Loaders:"
      ],
      "metadata": {
        "id": "w_nlVm5vBUG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def collate_fn(batch):\n",
        "  batch = sorted(batch, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "  input_values, labels, input_lengths = zip(*batch)\n",
        "  # print(input_values)\n",
        "  # print(labels)\n",
        "  # print(input_lengths)\n",
        "\n",
        "  input_values_padded = pad_sequence([iv.squeeze() for iv in input_values], batch_first=True)\n",
        "\n",
        "  labels_padded = pad_sequence(labels, batch_first=True)\n",
        "\n",
        "  input_lengths = torch.tensor([iv.shape[1] for iv in input_values], dtype=torch.long)\n",
        "\n",
        "  # print(input_values_padded)\n",
        "  # print(labels_padded)\n",
        "  # print(input_lengths)\n",
        "\n",
        "  return input_values_padded, labels_padded, input_lengths"
      ],
      "metadata": {
        "id": "GiuHnwX4uEl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "train_loader = DataLoader(custom_data_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "validation_loader = DataLoader(custom_val_data_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "IB2l1cCSiOOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating The Validation Loop:"
      ],
      "metadata": {
        "id": "kif6KU51Bjg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, data_loader, criterion, device):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_values, labels, input_lengths = batch\n",
        "      input_values, labels = input_values.to(device), labels.to(device)\n",
        "      output = model(input_values, input_lengths.cpu())\n",
        "      output_lengths = output.shape[1]\n",
        "      output_lengths = torch.full((output.shape[0],), output_lengths, dtype=torch.int64)\n",
        "\n",
        "      log_probs = torch.nn.functional.log_softmax(output, dim=2)\n",
        "      log_probs = log_probs.permute(1, 0, 2)\n",
        "      label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long, device=device)\n",
        "\n",
        "      loss = criterion(log_probs, labels, output_lengths, label_lengths)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  avg_loss = total_loss / len(data_loader)\n",
        "  return avg_loss"
      ],
      "metadata": {
        "id": "1v8BEL5Qa0ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating The Training Loop:"
      ],
      "metadata": {
        "id": "Fve9Hjr9B0l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = 10000\n",
        "\n",
        "def train(model, data_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "  global best_loss\n",
        "  model.to(device)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    batch_idx = 0\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "      input_values, labels, input_lengths = batch\n",
        "      input_values, labels = input_values.to(device), labels.to(device)\n",
        "\n",
        "      output = model(input_values, input_lengths.cpu())  # Added to the CPU; as it produced errors on GPU\n",
        "      # print(\"output:\", output.shape)\n",
        "      # print(\"labels:\", labels.shape)\n",
        "      output_lengths = output.shape[1]\n",
        "      output_lengths = torch.full((output.shape[0],), output_lengths, dtype=torch.int64)\n",
        "      # print(\"output_lengths\", output_lengths)\n",
        "      # print(\"input_lengths\", input_lengths)\n",
        "\n",
        "      log_probs = torch.nn.functional.log_softmax(output, dim=2) # Because CTC expects log softmax probabilities\n",
        "      log_probs = log_probs.permute(1, 0, 2) # The output of the network needs to be in the shape (output sequence length, batch, classes)\n",
        "\n",
        "      label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long, device=device)\n",
        "\n",
        "      # print(\"Output shape:\", output.shape)\n",
        "      # print(\"Labels:\", labels)\n",
        "      # print(\"Label lengths:\", label_lengths)\n",
        "      # print(output_lengths)\n",
        "      # print(log_probs.shape)\n",
        "      # print(label_lengths)\n",
        "      # print(labels.shape)\n",
        "\n",
        "      loss = criterion(log_probs, labels, output_lengths, label_lengths)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "      optimizer.step()\n",
        "\n",
        "      print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n",
        "      batch_idx+=1\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "\n",
        "    avg_val_loss = validate(model, val_loader, criterion, device)\n",
        "    print(\"validation loss: \", avg_val_loss)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      torch.save(model.state_dict(), f\"model_state_dict_epoch_{epoch}.pth\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "      best_loss = avg_loss\n",
        "      torch.save(model.state_dict(), \"best_model_state_dict.pth\")"
      ],
      "metadata": {
        "id": "2bpyqnh0jRwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Validation:"
      ],
      "metadata": {
        "id": "2VToVqVNETVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "XBBfXTwtjx5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CTCLoss(blank=processor.tokenizer.pad_token_id).to(device)"
      ],
      "metadata": {
        "id": "ZHqMXDO2kEw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(custom_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "co8bGpJMkFqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "train(custom_model, train_loader, validation_loader, criterion, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "f0ARqVFMjx9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7- Inference Phase:"
      ],
      "metadata": {
        "id": "YdRVuJJQGTMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model Weights:"
      ],
      "metadata": {
        "id": "8Uawe-SyGg8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomSTTModel(wav2vec_model, lstm_hidden_size=128, lstm_layers=2, attention_heads=4)\n",
        "model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "NkR8-cHFneT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Specific Model Outputs to Decode:"
      ],
      "metadata": {
        "id": "M5XhlzmPInqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_outputs(outputs, blank_label):\n",
        "  arg_maxes = torch.argmax(outputs, dim=2)\n",
        "  # print(arg_maxes.shape)\n",
        "  decodes = []\n",
        "  for i in range(arg_maxes.size(0)):\n",
        "    decode = []\n",
        "    for j in range(arg_maxes.size(1)):\n",
        "      if arg_maxes[i][j] != blank_label:\n",
        "        if j != 0 and arg_maxes[i][j-1] == arg_maxes[i][j]:\n",
        "          continue\n",
        "        decode.append(arg_maxes[i][j].item())\n",
        "    decodes.append(decode)\n",
        "  return decodes"
      ],
      "metadata": {
        "id": "Yv_jov-sAQ5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference:"
      ],
      "metadata": {
        "id": "b7ZhLeUCITqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import log_softmax"
      ],
      "metadata": {
        "id": "H-4TEOGYqpuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, processor, input_values, input_length, device):\n",
        "  model.eval()\n",
        "  input_values = input_values.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    logits = model(input_values, input_length)\n",
        "\n",
        "    log_probs = log_softmax(logits, dim=2)\n",
        "\n",
        "    decoded_preds = select_outputs(log_probs, blank_label=processor.tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_text = [processor.decode(pred) for pred in decoded_preds]\n",
        "\n",
        "  return decoded_text"
      ],
      "metadata": {
        "id": "_ISzNgE9qlS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio_file(file_path, processor, target_sample_rate=16000):\n",
        "  audio_input, sampling_rate = torchaudio.load(file_path)\n",
        "\n",
        "  if audio_input.shape[0] == 2: # Convert stereo to mono\n",
        "    audio_input = torch.mean(audio_input, dim=0, keepdim=True)\n",
        "\n",
        "  if sampling_rate != target_sample_rate:\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=target_sample_rate)\n",
        "    audio_input = resampler(audio_input)\n",
        "\n",
        "  input_values = processor(audio_input, sampling_rate=target_sample_rate, return_tensors=\"pt\").input_values\n",
        "  input_values = input_values.squeeze(0)\n",
        "  input_length = input_values.shape[1]\n",
        "  input_length = torch.tensor([input_length], dtype=torch.long)\n",
        "  return input_values, input_length"
      ],
      "metadata": {
        "id": "QwjWrCe-rxtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"r2.wav\"\n",
        "input_values, input_length = process_audio_file(file_path, processor)"
      ],
      "metadata": {
        "id": "wehIHFgrr9bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_values.shape)\n",
        "print(input_length)"
      ],
      "metadata": {
        "id": "pMewNchrsQrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(custom_model, processor, input_values, input_length, device)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "IzyQNejmqlU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
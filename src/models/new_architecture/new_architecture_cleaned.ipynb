{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Installing & Importing Necessary Packages:"
      ],
      "metadata": {
        "id": "oAvfbqQYhrDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio transformers datasets"
      ],
      "metadata": {
        "id": "FvbUVAiQijJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ffwp0rWhsD5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2Config, Wav2Vec2Model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import soundfile as sf\n",
        "import torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "CWZwYNzxiq4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Downloading & Processing Datasets:"
      ],
      "metadata": {
        "id": "vPcWUkFxjOP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading & Processing Common Voice 13 Dataset:"
      ],
      "metadata": {
        "id": "QuwkKlH2hyp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"ar\", split=\"train+test+validation\")"
      ],
      "metadata": {
        "id": "Ulv9pxsiisth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(common_voice))"
      ],
      "metadata": {
        "id": "533b7KB2TriI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice)"
      ],
      "metadata": {
        "id": "SH_ceoAMkiaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.remove_columns([\"client_id\", \"audio\", \"up_votes\", \"down_votes\", \"age\", \"gender\", \"accent\", \"locale\", \"segment\", \"variant\"])\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "UyikuA1IkicW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[0])"
      ],
      "metadata": {
        "id": "pltajkSTkif1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading & Processing Google Fleurs Dataset:"
      ],
      "metadata": {
        "id": "lC5SStQLjklt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_train = load_dataset(\"google/fleurs\", \"ar_eg\", split=\"train\")"
      ],
      "metadata": {
        "id": "dFLE7SVgFOxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_test = load_dataset(\"google/fleurs\", \"ar_eg\", split=\"test\")"
      ],
      "metadata": {
        "id": "1-e_Df13bXmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_val = load_dataset(\"google/fleurs\", \"ar_eg\", split=\"validation\")"
      ],
      "metadata": {
        "id": "wUivGSoCbXqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(fleurs_train))\n",
        "print(len(fleurs_test))\n",
        "print(len(fleurs_val))"
      ],
      "metadata": {
        "id": "UJ-1SSFdTvRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fleurs_train)"
      ],
      "metadata": {
        "id": "F1zlTcWDzorR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_audio_path_train(data_item):\n",
        "  parts = data_item[\"path\"].split('/')\n",
        "  parts.insert(-1, \"train\")\n",
        "  data_item[\"path\"] = '/'.join(parts)\n",
        "  data_item[\"sentence\"] = data_item[\"transcription\"]\n",
        "  return data_item\n",
        "def update_audio_path_test(data_item):\n",
        "  parts = data_item[\"path\"].split('/')\n",
        "  parts.insert(-1, \"test\")\n",
        "  data_item[\"path\"] = '/'.join(parts)\n",
        "  data_item[\"sentence\"] = data_item[\"transcription\"]\n",
        "  return data_item\n",
        "def update_audio_path_val(data_item):\n",
        "  parts = data_item[\"path\"].split('/')\n",
        "  parts.insert(-1, \"dev\")\n",
        "  data_item[\"path\"] = '/'.join(parts)\n",
        "  data_item[\"sentence\"] = data_item[\"transcription\"]\n",
        "  return data_item"
      ],
      "metadata": {
        "id": "Rx3UtY7kJoDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_train = fleurs_train.map(update_audio_path_train)"
      ],
      "metadata": {
        "id": "o2KbDOxWJst1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_test = fleurs_test.map(update_audio_path_test)"
      ],
      "metadata": {
        "id": "f2ZvNgUnbw4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_val = fleurs_val.map(update_audio_path_val)"
      ],
      "metadata": {
        "id": "JQVFQ4S0bw77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_train = fleurs_train.remove_columns([\"id\", \"num_samples\", \"audio\", \"transcription\", \"raw_transcription\", \"gender\", \"lang_id\", \"language\", \"lang_group_id\"])"
      ],
      "metadata": {
        "id": "pUhxS4HlHmSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_test = fleurs_test.remove_columns([\"id\", \"num_samples\", \"audio\", \"transcription\", \"raw_transcription\", \"gender\", \"lang_id\", \"language\", \"lang_group_id\"])"
      ],
      "metadata": {
        "id": "HZ4s-EvucGgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_val = fleurs_val.remove_columns([\"id\", \"num_samples\", \"audio\", \"transcription\", \"raw_transcription\", \"gender\", \"lang_id\", \"language\", \"lang_group_id\"])"
      ],
      "metadata": {
        "id": "PdqrgRA-cGjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fleurs_train[0])\n",
        "print(fleurs_test[0])\n",
        "print(fleurs_val[0])"
      ],
      "metadata": {
        "id": "tJ3l20Wrb7KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Combining Datasets & Creating The Vocabulary:"
      ],
      "metadata": {
        "id": "j9Q6HfogjqVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating The Combined Dataset For Training & The Vocabulary:"
      ],
      "metadata": {
        "id": "50WxIpr-j_9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = concatenate_datasets([common_voice, fleurs_train, fleurs_test])"
      ],
      "metadata": {
        "id": "SKErU1R8UAEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(common_voice))\n",
        "print(len(fleurs_train))\n",
        "print(len(fleurs_test))\n",
        "print(len(fleurs_val))\n",
        "print(len(combined_dataset))"
      ],
      "metadata": {
        "id": "ukz3Q8_qUVFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_dataset)"
      ],
      "metadata": {
        "id": "0uJ5BmgmUVNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_dataset[0])"
      ],
      "metadata": {
        "id": "rhoPnbd6UdYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = ['ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'آ', 'أ', 'إ', 'ؤ', 'ئ', 'ة', 'ى', 'ﻻ', 'ﻷ', 'ﻹ', 'ﻵ',' ', '.']"
      ],
      "metadata": {
        "id": "tuu99oLsr2d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocabulary))"
      ],
      "metadata": {
        "id": "9wUTYK0o3wO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing The Training & Testing Datasets:"
      ],
      "metadata": {
        "id": "VfLULN4AkXXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_transcriptions(data_item):\n",
        "  new_sentence = ''.join([char for char in data_item[\"sentence\"] if char in vocabulary])\n",
        "  data_item[\"sentence\"] = new_sentence\n",
        "  return data_item"
      ],
      "metadata": {
        "id": "tWCM_t5LKDj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = combined_dataset.map(process_transcriptions)"
      ],
      "metadata": {
        "id": "9yizyGBB_0pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_dataset[0])"
      ],
      "metadata": {
        "id": "m7giwxi_VuFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs_val = fleurs_val.map(process_transcriptions)"
      ],
      "metadata": {
        "id": "xkCDas7SL-X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Creating The Custom Processor:"
      ],
      "metadata": {
        "id": "RCZYVFYCkdcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is used to map characters to integers and vice versa."
      ],
      "metadata": {
        "id": "MzNTeXwWkho4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomProcessor:\n",
        "  def __init__(self, vocab):\n",
        "    self.vocab = vocab\n",
        "    self.char_to_index = {char: index for index, char in enumerate(vocab)}\n",
        "\n",
        "  def text_to_int(self, text):\n",
        "    return [self.char_to_index[char] for char in text]\n",
        "\n",
        "  def int_to_text(self, indices):\n",
        "    return ''.join([self.vocab[index] for index in indices])"
      ],
      "metadata": {
        "id": "2PvG23G-ohfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_voc = CustomProcessor(vocabulary)"
      ],
      "metadata": {
        "id": "KngcYCUMu62g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"مرحبا\"\n",
        "encoding = object_voc.text_to_int(text)\n",
        "decoding = object_voc.int_to_text(encoding)\n",
        "print(encoding)\n",
        "print(decoding)"
      ],
      "metadata": {
        "id": "mb4oPAWMu_sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- Creating The Dataset Class:"
      ],
      "metadata": {
        "id": "YaszVk8ukwOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio"
      ],
      "metadata": {
        "id": "sgE-Z2bXooZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, vocab):\n",
        "    self.dataset = dataset\n",
        "    self.processor = CustomProcessor(vocab)\n",
        "    self.spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    audio_input, sampling_rate = torchaudio.load(self.dataset[idx][\"path\"])\n",
        "    if sampling_rate != 16000:\n",
        "      resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
        "      audio_input = resampler(audio_input)\n",
        "\n",
        "    # spectrogram = self.spectrogram_transform(audio_input).squeeze(0)\n",
        "    spectrogram = self.spectrogram_transform(audio_input)\n",
        "    spectrogram = (spectrogram - spectrogram.mean()) / spectrogram.std()\n",
        "\n",
        "    sentence = self.dataset[idx][\"sentence\"]\n",
        "\n",
        "    labels = self.processor.text_to_int(sentence)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return spectrogram, labels, spectrogram.shape[-1]"
      ],
      "metadata": {
        "id": "e5MJNopxiOJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_data_set = CustomDataset(combined_dataset, vocabulary)\n",
        "custom_test_data_set = CustomDataset(fleurs_val, vocabulary)"
      ],
      "metadata": {
        "id": "wxp3e6u0iOLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6- Model Architecture:"
      ],
      "metadata": {
        "id": "YDsxe-Mrk36H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "id": "naEHRy_digHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "Nnvy412WxKDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomSTTModel2(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(CustomSTTModel2, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    self.bn3 = nn.BatchNorm2d(128)\n",
        "    self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    self.bn4 = nn.BatchNorm2d(256)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.skip_conv2 = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "    self.skip_conv3 = nn.Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "    self.skip_conv4 = nn.Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "    self.pos_encoder = PositionalEncoding(d_model=8192)\n",
        "\n",
        "    self.gru1 = nn.GRU(input_size=8192, hidden_size=128, num_layers=1, bidirectional=True, batch_first=True)\n",
        "    self.gru2 = nn.GRU(input_size=256, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    self.fc = nn.Linear(512, 256)\n",
        "    self.out = nn.Linear(256, num_classes + 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connection = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = nn.ELU()(x)\n",
        "    # x is torch.Size([8, 32, 128, 490])\n",
        "    x = x + skip_connection  # Element-wise addition (broadcasting works)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    skip_connection = self.skip_conv2(x) # Adjusting skip_connection to be able to add it with x\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = nn.ELU()(x)\n",
        "    x = x + skip_connection\n",
        "    x = self.pool(x)\n",
        "\n",
        "    # print(x.shape)\n",
        "    # torch.Size([8, 64, 32, 166]) [Batch Size, Channels, Height(frequency), Width(time)]\n",
        "\n",
        "    skip_connection = self.skip_conv3(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = nn.ELU()(x)\n",
        "    x = x + skip_connection\n",
        "    # x = self.pool(x) Removed due to introducing numerical instability\n",
        "    # print(x.shape)\n",
        "    # print(skip_connection.shape)\n",
        "    # torch.Size([8, 128, 32, 166]) [Batch Size, Channels, Height(frequency), Width(time)]\n",
        "\n",
        "    skip_connection = self.skip_conv4(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    x = nn.ELU()(x)\n",
        "    x = x + skip_connection\n",
        "    # x = self.pool(x) Removed due to numerical instability\n",
        "    # print(x.shape)\n",
        "    # torch.Size([8, 256, 32, 166]) [Batch Size, Channels, Height(frequency), Width(time)]\n",
        "    x = x.permute(0, 3, 1, 2)  # Rearrange the dimensions for GRU\n",
        "\n",
        "    # print(x.shape)\n",
        "    # torch.Size([8, 166, 256, 32]) [Batch Size, Width, Channels, Height]\n",
        "    x = torch.flatten(x, start_dim=2)  # Flatten the non-time dimensions (channels and frequency bins) into a single feature dimension for GRU as well\n",
        "    # print(x.shape)\n",
        "    # torch.Size([8, 166, 8192]) [Batch Size, Width, Channels * Height]\n",
        "\n",
        "    x = self.pos_encoder(x)\n",
        "\n",
        "    # print(x.shape)\n",
        "\n",
        "    x, _ = self.gru1(x)\n",
        "    # print(x.shape)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x, _ = self.gru2(x)\n",
        "    # print(x.shape)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "\n",
        "    x = self.fc(x)\n",
        "    x = nn.ELU()(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.out(x)\n",
        "    # print(x.shape)\n",
        "    # print(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Wl6zvxijH7yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model_2 = CustomSTTModel2(len(vocabulary))"
      ],
      "metadata": {
        "id": "UqIc02qnh0EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7- Training Phase:"
      ],
      "metadata": {
        "id": "hoXL_RTAvdsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RekVyTcFxVpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating The Collate Function & Initializing Train & Test Loaders:"
      ],
      "metadata": {
        "id": "cTr6sbvnvfyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "  spectrograms, labels, input_lengths = zip(*batch)\n",
        "\n",
        "  spectrograms = [s.permute(2, 0, 1) for s in spectrograms]  # Shape: [time_steps, channels, n_mels] since we need to pad the time_steps and pad_sequence pads the first dimension\n",
        "\n",
        "  spectrograms = pad_sequence(spectrograms, batch_first=True, padding_value=0) # Pad the spectrograms to have the same time length\n",
        "\n",
        "  spectrograms = spectrograms.permute(0, 2, 3, 1)  # Shape: [batch, channels, n_mels, time_steps] we just return back the original dimensions\n",
        "\n",
        "  label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long, device=device)\n",
        "\n",
        "  labels = torch.cat(labels)\n",
        "\n",
        "  input_lengths = torch.tensor(input_lengths)\n",
        "\n",
        "  return spectrograms, labels, input_lengths, label_lengths"
      ],
      "metadata": {
        "id": "WxZY-P_F5K_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "train_loader = DataLoader(custom_data_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(custom_test_data_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "IB2l1cCSiOOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating The Testing Loop:"
      ],
      "metadata": {
        "id": "BDCYv1LQwMYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, data_loader, criterion, device):\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (spectrograms, labels, input_lengths, label_lengths) in enumerate(data_loader):\n",
        "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "      outputs = model(spectrograms)\n",
        "      output_lengths = outputs.shape[1]\n",
        "      output_lengths = torch.full((outputs.shape[0],), output_lengths, dtype=torch.int64)\n",
        "\n",
        "      log_probs = torch.nn.functional.log_softmax(outputs, dim=2)\n",
        "      log_probs = log_probs.permute(1, 0, 2).to(device)\n",
        "      label_lengths = label_lengths.to(device)\n",
        "      output_lengths = output_lengths.to(device)\n",
        "\n",
        "      loss = criterion(log_probs, labels, output_lengths, label_lengths)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  avg_loss = total_loss / len(data_loader)\n",
        "  return avg_loss"
      ],
      "metadata": {
        "id": "MBGlmyLCKTDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating The Training Loop:"
      ],
      "metadata": {
        "id": "InNklLZywY-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = 10000\n",
        "\n",
        "def train(model, data_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "  global best_loss\n",
        "  model.to(device)\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (spectrograms, labels, input_lengths, label_lengths) in enumerate(train_loader):\n",
        "      spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "      outputs = model(spectrograms) # [batch, output sequence length, classes]\n",
        "      output_lengths = outputs.shape[1]\n",
        "      output_lengths = torch.full((outputs.shape[0],), output_lengths, dtype=torch.int64)\n",
        "      # print(outputs)\n",
        "      log_probs = torch.nn.functional.log_softmax(outputs, dim=2) # Because CTC expects log softmax probabilities\n",
        "      # print(log_probs.shape)\n",
        "\n",
        "      log_probs = log_probs.permute(1, 0, 2) # The output of the network needs to be in the shape (output sequence length, batch, classes)\n",
        "      # print(log_probs.shape)\n",
        "      log_probs = log_probs.to(device)\n",
        "      label_lengths = label_lengths.to(device)\n",
        "      output_lengths = output_lengths.to(device)\n",
        "      # print(output_lengths)\n",
        "      # print(log_probs.shape)\n",
        "      # print(label_lengths)\n",
        "      # print(labels.shape)\n",
        "\n",
        "      loss = criterion(log_probs, labels, output_lengths, label_lengths)\n",
        "      if torch.isinf(loss): # This is due to a few numerical instabilities.\n",
        "        # print(output_lengths)\n",
        "        # print(log_probs.shape)\n",
        "        # print(label_lengths)\n",
        "        # print(labels.shape)\n",
        "        # print(outputs)\n",
        "        # print(log_probs)\n",
        "        continue\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "      optimizer.step()\n",
        "\n",
        "      print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "\n",
        "    avg_val_loss = test(model, val_loader, criterion, device)\n",
        "    print(\"validation loss: \", avg_val_loss)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      torch.save(model.state_dict(), f\"model_state_dict_epoch_{epoch}.pth\")\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "      best_loss = avg_loss\n",
        "      torch.save(model.state_dict(), \"best_model_state_dict.pth\")"
      ],
      "metadata": {
        "id": "2bpyqnh0jRwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Testing:"
      ],
      "metadata": {
        "id": "s9MJyb7Dy_Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CTCLoss(blank=len(vocabulary)).to(device)"
      ],
      "metadata": {
        "id": "ZHqMXDO2kEw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(custom_model_2.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "co8bGpJMkFqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "train(custom_model_2, train_loader, test_loader, criterion, optimizer, epochs, device)"
      ],
      "metadata": {
        "id": "f0ARqVFMjx9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8- Inference Phase:"
      ],
      "metadata": {
        "id": "_91pftZTzDtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Audio-To-Spectrogram Transformation:"
      ],
      "metadata": {
        "id": "Q0Zig1cpzda5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_audio_to_spectrogram(audio_path, transform):\n",
        "  waveform, sample_rate = torchaudio.load(audio_path)\n",
        "  if waveform.shape[0] == 2: # Convert stereo to mono\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "  if sample_rate != 16000:\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "    waveform = resampler(waveform)\n",
        "\n",
        "  spectrogram = transform(waveform)\n",
        "  spectrogram = (spectrogram - spectrogram.mean()) / spectrogram.std()\n",
        "  return spectrogram"
      ],
      "metadata": {
        "id": "9DrQzvKe6h1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spectrogram_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)"
      ],
      "metadata": {
        "id": "FBLf4xT6jdbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model Weights:"
      ],
      "metadata": {
        "id": "3OEJlTdYzjvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"best_model_state_dict.pth\"\n",
        "model = CustomSTTModel2(len(vocabulary))\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "KIfvvjmjjDpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference:"
      ],
      "metadata": {
        "id": "914XHRMQznhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(audio_path, model, transform, processor):\n",
        "  spectrogram = transform_audio_to_spectrogram(audio_path, transform)\n",
        "  spectrogram = spectrogram.unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(spectrogram)\n",
        "    outputs = torch.nn.functional.log_softmax(outputs, dim=2)\n",
        "    outputs = outputs.permute(1, 0, 2)\n",
        "\n",
        "  decoded_preds = torch.argmax(outputs, dim=2)\n",
        "  decoded_preds = decoded_preds.transpose(0, 1)\n",
        "\n",
        "  decoded_preds_list = decoded_preds.flatten().tolist()\n",
        "  # print(decoded_preds_list)\n",
        "  pred_text = processor.int_to_text([i for i in decoded_preds_list if i < len(processor.vocab)])\n",
        "\n",
        "  return pred_text"
      ],
      "metadata": {
        "id": "osoWZAVgjgCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_voc = CustomProcessor(vocabulary)\n",
        "audio_file = \"r3.wav\"\n",
        "# audio_file = dataset[0][\"path\"]\n",
        "predicted_text = infer(audio_file, model, spectrogram_transform, object_voc)\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "id": "VnR0VIzJjjBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5FOWwP9EksC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}